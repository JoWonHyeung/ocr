{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d13862e3-bb27-47af-9b58-a9fbf804df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7df3f2-62d0-4499-a46e-47d01699def0",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3367399-9798-4e38-967b-fd2320b9a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_HEIGHT_SIZE':64,\n",
    "    'IMG_WIDTH_SIZE':224,\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':1e-3,\n",
    "    'BATCH_SIZE':256,\n",
    "    'NUM_WORKERS':0, # 본인의 GPU, CPU 환경에 맞게 설정\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254e860-ff82-43ba-bfa3-fcee4eb3ddbd",
   "metadata": {},
   "source": [
    "## Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "101a714b-71b6-4475-a4ce-fa5f98bc2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {},
   "source": [
    "## Data Load & Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a62c78cd-4f40-4e98-b8a6-1b6f1d906b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf1e9b48-74b0-4467-bd23-5e67b4ad0764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제공된 학습데이터 중 1글자 샘플들의 단어사전이 학습/테스트 데이터의 모든 글자를 담고 있으므로 학습 데이터로 우선 배치\n",
    "df['len'] = df['label'].str.len()\n",
    "train_v1 = df[df['len']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64ef1d10-8f7d-4807-aec4-728bf08a2eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제공된 학습데이터 중 2글자 이상의 샘플들에 대해서 단어길이를 고려하여 Train (80%) / Validation (20%) 분할\n",
    "df = df[df['len']>1]\n",
    "train_v2, val, _, _ = train_test_split(df, df['len'], test_size=0.2, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b53142c6-4d17-44a1-a0c1-c23ee7f83f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66251 10637\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터로 우선 배치한 1글자 샘플들과 분할된 2글자 이상의 학습 샘플을 concat하여 최종 학습 데이터로 사용\n",
    "train = pd.concat([train_v1, train_v2])\n",
    "print(len(train), len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc6f99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = \"C:/Users/Jo/PYDATAexam/train/\" \n",
    "train = pd.read_csv('C:/Users/Jo/PYDATAexam/train.csv')\n",
    "\n",
    "df['img_path'] = train['img_path'].apply(lambda x: default_path + x.split('/')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1c27566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_00000.png</td>\n",
       "      <td>빨간색</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_00002.png</td>\n",
       "      <td>차차</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_00004.png</td>\n",
       "      <td>놓치다</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TRAIN_00005</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_00005.png</td>\n",
       "      <td>오래도록</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TRAIN_00006</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_00006.png</td>\n",
       "      <td>유월</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76881</th>\n",
       "      <td>TRAIN_76881</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_76881.png</td>\n",
       "      <td>구분하다</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76882</th>\n",
       "      <td>TRAIN_76882</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_76882.png</td>\n",
       "      <td>하나하나</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76884</th>\n",
       "      <td>TRAIN_76884</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_76884.png</td>\n",
       "      <td>겪다</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76885</th>\n",
       "      <td>TRAIN_76885</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_76885.png</td>\n",
       "      <td>벨트</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76887</th>\n",
       "      <td>TRAIN_76887</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_76887.png</td>\n",
       "      <td>자리</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53185 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                      img_path label  len\n",
       "0      TRAIN_00000  C:/Users/Jo/PYDATAexam/train/TRAIN_00000.png   빨간색    3\n",
       "2      TRAIN_00002  C:/Users/Jo/PYDATAexam/train/TRAIN_00002.png    차차    2\n",
       "4      TRAIN_00004  C:/Users/Jo/PYDATAexam/train/TRAIN_00004.png   놓치다    3\n",
       "5      TRAIN_00005  C:/Users/Jo/PYDATAexam/train/TRAIN_00005.png  오래도록    4\n",
       "6      TRAIN_00006  C:/Users/Jo/PYDATAexam/train/TRAIN_00006.png    유월    2\n",
       "...            ...                                           ...   ...  ...\n",
       "76881  TRAIN_76881  C:/Users/Jo/PYDATAexam/train/TRAIN_76881.png  구분하다    4\n",
       "76882  TRAIN_76882  C:/Users/Jo/PYDATAexam/train/TRAIN_76882.png  하나하나    4\n",
       "76884  TRAIN_76884  C:/Users/Jo/PYDATAexam/train/TRAIN_76884.png    겪다    2\n",
       "76885  TRAIN_76885  C:/Users/Jo/PYDATAexam/train/TRAIN_76885.png    벨트    2\n",
       "76887  TRAIN_76887  C:/Users/Jo/PYDATAexam/train/TRAIN_76887.png    자리    2\n",
       "\n",
       "[53185 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54634f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "val['img_path'] = val['img_path'].apply(lambda x: default_path + x.split('/')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f79cd9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = \"C:/Users/Jo/PYDATAexam/test/\" \n",
    "test = pd.read_csv(\"C:/Users/Jo/PYDATAexam/test.csv\")\n",
    "\n",
    "test['img_path'] = test['img_path'].apply(lambda x: default_path + x.split('/')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43b671-f8a5-403a-b2aa-7f779f6fd85a",
   "metadata": {},
   "source": [
    "## Get Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a384ae9-fc56-4660-96c2-0d424f64ea6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "805\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터로부터 단어 사전(Vocabulary) 구축\n",
    "train_gt = [gt for gt in df['label']]\n",
    "train_gt = \"\".join(train_gt)\n",
    "letters = sorted(list(set(list(train_gt))))\n",
    "print(len(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be0b36a1-bc45-4e2c-9f0a-e8f3e845299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [\"-\"] + letters\n",
    "print(len(vocabulary))\n",
    "idx2char = {k:v for k,v in enumerate(vocabulary, start=0)}\n",
    "char2idx = {v:k for k,v in idx2char.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27ed36-8031-47a7-bd0d-a913513f2e8e",
   "metadata": {},
   "source": [
    "## CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16fd60a5-24e2-4539-bfd0-1c374a641699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, train_mode=True):\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "        self.train_mode = train_mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_path_list[index]).convert('RGB')\n",
    "        \n",
    "        if self.train_mode:\n",
    "            image = self.train_transform(image)\n",
    "        else:\n",
    "            image = self.test_transform(image)\n",
    "            \n",
    "        if self.label_list is not None:\n",
    "            text = self.label_list[index]\n",
    "            return image, text\n",
    "        else:\n",
    "            return image\n",
    "    \n",
    "    # Image Augmentation\n",
    "    def train_transform(self, image):\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.Resize((CFG['IMG_HEIGHT_SIZE'],CFG['IMG_WIDTH_SIZE'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        return transform_ops(image)\n",
    "    \n",
    "    def test_transform(self, image):\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.Resize((CFG['IMG_HEIGHT_SIZE'],CFG['IMG_WIDTH_SIZE'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        return transform_ops(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d880481-1965-499d-9caa-fdfa8526f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df['img_path'].values, df['label'].values)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val['img_path'].values, val['label'].values)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bcee282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_00000.png</td>\n",
       "      <td>빨간색</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_00002.png</td>\n",
       "      <td>차차</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_00004.png</td>\n",
       "      <td>놓치다</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TRAIN_00005</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_00005.png</td>\n",
       "      <td>오래도록</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TRAIN_00006</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_00006.png</td>\n",
       "      <td>유월</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                      img_path label  len\n",
       "0  TRAIN_00000  C:/Users/Jo/PYDATAexam/train/TRAIN_00000.png   빨간색    3\n",
       "2  TRAIN_00002  C:/Users/Jo/PYDATAexam/train/TRAIN_00002.png    차차    2\n",
       "4  TRAIN_00004  C:/Users/Jo/PYDATAexam/train/TRAIN_00004.png   놓치다    3\n",
       "5  TRAIN_00005  C:/Users/Jo/PYDATAexam/train/TRAIN_00005.png  오래도록    4\n",
       "6  TRAIN_00006  C:/Users/Jo/PYDATAexam/train/TRAIN_00006.png    유월    2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3eccac93-a67b-4b45-908a-2283131fd5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3, 64, 224]) ('치료', '회장', '부정', '길이', '매일', '생선', '잔디밭', '작은딸', '완성하다', '고맙다', '부잣집', '놔두다', '이웃', '첫째', '소비자', '방문', '기혼', '노동자', '미용실', '인제', '축구공', '주름살', '전개되다', '개월', '원서', '위험하다', '어쩌다', '식생활', '사물', '분홍색', '버리다', '축구장', '담당하다', '뒤늦다', '예매하다', '찍다', '자장면', '정해지다', '아무', '자장면', '어색하다', '년생', '백색', '흥분하다', '기획', '이웃집', '자극하다', '두부', '와인', '위험하다', '그야말로', '일하다', '진행되다', '여행', '이렇다', '뜻대로', '본격적', '생신', '다음', '변호사', '특별하다', '정신적', '말씀', '바깥', '재미', '미술관', '초보자', '연기되다', '의미하다', '서울', '쓰다듬다', '넘어뜨리다', '걸리다', '빗방울', '상표', '힘들어하다', '절약', '심각해지다', '시골', '레이저', '통합', '일찍이', '넘어뜨리다', '일주일', '당연하다', '유명하다', '년생', '앞서다', '여군', '바닥', '특이하다', '정리되다', '상대', '중단', '바퀴', '모범', '어렵다', '방울', '내지', '위로', '다듬다', '저축', '열차', '드디어', '불법', '적극적', '이사', '힘들다', '참석하다', '장난', '조미료', '붙이다', '구하다', '사립', '구별되다', '아하', '선진국', '고집하다', '라디오', '올바르다', '능력', '이해', '한국적', '연관', '입장', '태우다', '거기', '소형', '나흘', '간단히', '학번', '삼계탕', '인구', '한꺼번에', '쓰다', '끊임없이', '꾸다', '묻히다', '부족', '노력', '짐작하다', '소매', '간장', '멍멍', '관계없이', '예방하다', '불편하다', '지하', '거치다', '의심', '온종일', '장식', '전공하다', '일찍', '아파트', '그루', '케첩', '여인', '멍멍', '의미하다', '고집', '전철', '틀림없다', '외출하다', '눈감다', '과거', '추위', '부동산', '불필요하다', '구멍', '원인', '하나', '기숙사', '사전', '물질적', '부르다', '금지되다', '뒤쪽', '섹시하다', '성숙하다', '그래서', '불행', '바싹', '물건', '추가되다', '넘어뜨리다', '욕하다', '소득', '운동화', '악몽', '개방', '타다', '희생', '대다', '경쟁', '예약', '그러나', '조르다', '화장', '기숙사', '반짝거리다', '알코올', '욕실', '활발해지다', '늘어놓다', '붙이다', '낙엽', '도리어', '내밀다', '사상', '안녕하다', '최대', '영업', '가만있다', '근교', '덮이다', '무기', '발휘하다', '대규모', '탄생', '푸다', '한국어', '대사', '솜씨', '다양성', '기본적', '묻다', '거절하다', '정상', '출퇴근', '대신', '안주', '감정', '어리석다', '과제', '반영하다', '나서다', '구입', '자랑스럽다', '되게', '평화', '유월', '분주하다', '행하다', '불리다', '모르다', '인연', '세로', '걱정', '보이다', '세상에', '간장', '한꺼번에', '잘하다', '여우', '간판')\n"
     ]
    }
   ],
   "source": [
    "image_batch, text_batch = next(iter(train_loader))\n",
    "print(image_batch.size(), text_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39962463-032f-490a-a76d-c03991795f38",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3664c4d0-f1f2-4971-9090-4d6ee66309ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "class RecognitionModel(nn.Module):\n",
    "    def __init__(self, num_chars=len(char2idx), rnn_hidden_size=256):\n",
    "        super(RecognitionModel, self).__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        # CNN Backbone = 사전\n",
    "        #학습된 resnet18 활용\n",
    "        # https://arxiv.org/abs/1512.03385\n",
    "        #resnet = resnet18(pretrained=True)\n",
    "        # CNN Feature Extract\n",
    "        one=list(model.children())[0]\n",
    "        two=list(model.children())[1]\n",
    "        three=list(model.children())[2]\n",
    "        four=list(model.children())[3:]\n",
    "        self.feature_extract = nn.Sequential(\n",
    "            one,\n",
    "            two,\n",
    "            *three\n",
    "            #nn.Conv2d(256, 256, kernel_size=(3,6), stride=1, padding=1),\n",
    "            #nn.BatchNorm2d(256),\n",
    "            #nn.ReLU(inplace=True)\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.linear1 = nn.Linear(640, rnn_hidden_size)\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(input_size=rnn_hidden_size, \n",
    "                            hidden_size=rnn_hidden_size,\n",
    "                            bidirectional=True, \n",
    "                            batch_first=True)\n",
    "        self.linear2 = nn.Linear(self.rnn_hidden_size*2, num_chars)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # CNN\n",
    "        x = self.feature_extract(x) # [batch_size, channels, height, width]\n",
    "        x = x.permute(0, 3, 1, 2) # [batch_size, width, channels, height]\n",
    "        batch_size = x.size(0)\n",
    "        T = x.size(1)\n",
    "        x = x.view(batch_size, T, -1) # [batch_size, T==width, num_features==channels*height]\n",
    "        #print(x.size())\n",
    "        x = self.linear1(x)\n",
    "        #print(x.size())\n",
    "        # RNN\n",
    "        x, hidden = self.rnn(x)\n",
    "        \n",
    "        output = self.linear2(x)\n",
    "        output = output.permute(1, 0, 2) # [T==10, batch_size, num_classes==num_features]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f5723-1137-4b6d-be4a-47b1c99e2744",
   "metadata": {},
   "source": [
    "## Define CTC Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32a34330-d5cd-4703-8930-45bd238e04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CTCLoss(blank=0) # idx 0 : '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dda15898-58b3-4431-8aa2-09cc2ea1c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_batch(text_batch):\n",
    "    text_batch_targets_lens = [len(text) for text in text_batch]\n",
    "    text_batch_targets_lens = torch.IntTensor(text_batch_targets_lens)\n",
    "    \n",
    "    text_batch_concat = \"\".join(text_batch)\n",
    "    text_batch_targets = [char2idx[c] for c in text_batch_concat]\n",
    "    text_batch_targets = torch.IntTensor(text_batch_targets)\n",
    "    \n",
    "    return text_batch_targets, text_batch_targets_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2bddfe3-30b0-42a2-8954-a8a19457be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(text_batch, text_batch_logits):\n",
    "    \"\"\"\n",
    "    text_batch: list of strings of length equal to batch size\n",
    "    text_batch_logits: Tensor of size([T, batch_size, num_classes])\n",
    "    \"\"\"\n",
    "    text_batch_logps = F.log_softmax(text_batch_logits, 2) # [T, batch_size, num_classes]  \n",
    "    text_batch_logps_lens = torch.full(size=(text_batch_logps.size(1),), \n",
    "                                       fill_value=text_batch_logps.size(0), \n",
    "                                       dtype=torch.int32).to(device) # [batch_size] \n",
    "\n",
    "    text_batch_targets, text_batch_targets_lens = encode_text_batch(text_batch)\n",
    "    loss = criterion(text_batch_logps, text_batch_targets, text_batch_logps_lens, text_batch_targets_lens)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122af0aa-a1fd-4595-9488-35761e3cb596",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a17df6b3-16c9-44dd-b0fd-ffb501fee749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    \n",
    "    best_loss = 999999\n",
    "    best_model = None\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for image_batch, text_batch in tqdm(iter(train_loader)):\n",
    "            image_batch = image_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            text_batch_logits = model(image_batch)\n",
    "            loss = compute_loss(text_batch, text_batch_logits)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        _train_loss = np.mean(train_loss)\n",
    "        \n",
    "        _val_loss = validation(model, val_loader, device)\n",
    "        print(f'Epoch : [{epoch}] Train CTC Loss : [{_train_loss:.5f}] Val CTC Loss : [{_val_loss:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_loss)\n",
    "        \n",
    "        if best_loss > _val_loss:\n",
    "            best_loss = _val_loss\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb26e02-35de-418b-8948-7447f5822cfe",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18efe906-aeb7-45a6-8db5-aed806fd7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for image_batch, text_batch in tqdm(iter(val_loader)):\n",
    "            image_batch = image_batch.to(device)\n",
    "            \n",
    "            text_batch_logits = model(image_batch)\n",
    "            loss = compute_loss(text_batch, text_batch_logits)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "    \n",
    "    _val_loss = np.mean(val_loss)\n",
    "    return _val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da39f9-904f-4abd-a7d2-cdf29c4a6c24",
   "metadata": {},
   "source": [
    "## Run!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86142d9a-68b7-4d04-8423-49d28025411d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9867babdecf4a7d95b3536ceccd1fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3083050b154a411ba32f4d5cd12591aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1] Train CTC Loss : [0.04241] Val CTC Loss : [0.05851]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3667fa87dd4c4d449ca1ccf6b5576ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596f46b29c584960b18f0fbbb40abd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [2] Train CTC Loss : [0.03973] Val CTC Loss : [0.06129]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0923f003374df290c39b9b3be95732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a2590e62714a6d9593ac85474cb80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [3] Train CTC Loss : [0.03386] Val CTC Loss : [0.04862]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa99affda454fdb91865aaeef3dbc5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4331e72db9ac4039bb219616026419ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [4] Train CTC Loss : [0.02771] Val CTC Loss : [0.02637]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f574d5f6a7c74bb28139fc397af95f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a19c85bc904fb0a7ecfa99197db6cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [5] Train CTC Loss : [0.03095] Val CTC Loss : [0.03425]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f1c29fc514493ba1ae98f04b23a2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f901813103ac4459b3475c695874bcd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [6] Train CTC Loss : [0.03143] Val CTC Loss : [0.04376]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5d5a4215a144b7808603e4ed042680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadf140cad7847e3be5efa3b17b9af7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [7] Train CTC Loss : [0.02663] Val CTC Loss : [0.02429]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44708fe4fe76486cb1f9a7696006b34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfed8dbe57f4d0eb65b1015cae5818f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [8] Train CTC Loss : [0.02337] Val CTC Loss : [0.05572]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75f4529786149e1904ec44e72e4499a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c33479dfc19408f8d728d22e30bec0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [9] Train CTC Loss : [0.02861] Val CTC Loss : [0.04019]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b3a6bf0a914778a15c2b62ac969d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc21d79a7e943abab41fa00860e430b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [10] Train CTC Loss : [0.03298] Val CTC Loss : [0.02874]\n",
      "Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    }
   ],
   "source": [
    "#model = RecognitionModel()\n",
    "model = torch.load(\"./model.pt\")\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b807d-ac45-4183-b92d-2b40f154e66c",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23e20fdc-ce3e-49c6-899d-317dde3316c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0854f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = \"C:/Users/Jo/PYDATAexam/test/\" \n",
    "test = pd.read_csv(\"C:/Users/Jo/PYDATAexam/test.csv\")\n",
    "\n",
    "test['img_path'] = test['img_path'].apply(lambda x: default_path + x.split('/')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5705b13-5246-4519-b11d-75102322c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test['img_path'].values, None, False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=CFG['NUM_WORKERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bd93adb-adfc-4376-ad73-ed8fc156599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(text_batch_logits):\n",
    "    text_batch_tokens = F.softmax(text_batch_logits, 2).argmax(2) # [T, batch_size]\n",
    "    text_batch_tokens = text_batch_tokens.numpy().T # [batch_size, T]\n",
    "\n",
    "    text_batch_tokens_new = []\n",
    "    for text_tokens in text_batch_tokens:\n",
    "        text = [idx2char[idx] for idx in text_tokens]\n",
    "        text = \"\".join(text)\n",
    "        text_batch_tokens_new.append(text)\n",
    "\n",
    "    return text_batch_tokens_new\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for image_batch in tqdm(iter(test_loader)):\n",
    "            image_batch = image_batch.to(device)\n",
    "            \n",
    "            text_batch_logits = model(image_batch)\n",
    "            \n",
    "            text_batch_pred = decode_predictions(text_batch_logits.cpu())\n",
    "            \n",
    "            preds.extend(text_batch_pred)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57711ef8-048b-481d-8c98-759a9c81f2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b721c5b07cd4dcda65e33eb20ece00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = inference(infer_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3904ed-61af-4db6-896c-2bcd21709b9f",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87e53da9-eb17-4e9a-a5d7-857143128961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 별 추론결과를 독립적으로 후처리\n",
    "def remove_duplicates(text):\n",
    "    if len(text) > 1:\n",
    "        letters = [text[0]] + [letter for idx, letter in enumerate(text[1:], start=1) if text[idx] != text[idx-1]]\n",
    "    elif len(text) == 1:\n",
    "        letters = [text[0]]\n",
    "    else:\n",
    "        return \"\"\n",
    "    return \"\".join(letters)\n",
    "\n",
    "def correct_prediction(word):\n",
    "    parts = word.split(\"-\")\n",
    "    parts = [remove_duplicates(part) for part in parts]\n",
    "    corrected_word = \"\".join(parts)\n",
    "    return corrected_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55c6f77c-afe4-4a75-848f-757e498f2888",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../sample_submission.csv')\n",
    "submit['label'] = predictions\n",
    "submit['label'] = submit['label'].apply(correct_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d5f4863-49d8-4e90-92c8-b500e0b275d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
