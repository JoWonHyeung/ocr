{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0538b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_TRAINING = False\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pynvml import *\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "    \n",
    "default_path = \"C:/Users/Jo/PYDATAexam/train/\" \n",
    "df = pd.read_csv('../train.csv')\n",
    "\n",
    "df.rename(columns={'img_path': \"file_name\", \"label\": \"text\"}, inplace=True)\n",
    "df['text'] = df['text'].str.strip()\n",
    "df['file_name'] = df['file_name'].apply(lambda x: default_path + x.split('/')[2])\n",
    "\n",
    "# Just for hands-on lab\n",
    "if not FULL_TRAINING:\n",
    "    df = df.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b277bb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a90eeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Jo/.cache\\huggingface\\hub\\models--daekeun-ml--ko-trocr-base-nsmc-news-chatbot\\snapshots\\b8e0ab4b7ef7c686a23654e24ffd6ebe36bb59da\\config.json\n",
      "Model config VisionEncoderDecoderConfig {\n",
      "  \"_commit_hash\": \"b8e0ab4b7ef7c686a23654e24ffd6ebe36bb59da\",\n",
      "  \"_name_or_path\": \"daekeun-ml/ko-trocr-base-nsmc-news-chatbot\",\n",
      "  \"architectures\": [\n",
      "    \"VisionEncoderDecoderModel\"\n",
      "  ],\n",
      "  \"decoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"RobertaForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 0,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": null,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"gradient_checkpointing\": false,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 514,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"roberta\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 1,\n",
      "    \"position_embedding_type\": \"absolute\",\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": \"BertTokenizer\",\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.25.1\",\n",
      "    \"type_vocab_size\": 1,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 32000\n",
      "  },\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"DeiTForImageClassificationWithTeacher\"\n",
      "    ],\n",
      "    \"attention_probs_dropout_prob\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"encoder_stride\": 16,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.0,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"image_size\": 384,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"deit\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"patch_size\": 16,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"qkv_bias\": true,\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.25.1\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 64,\n",
      "  \"model_type\": \"vision-encoder-decoder\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": null,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Jo/.cache\\huggingface\\hub\\models--daekeun-ml--ko-trocr-base-nsmc-news-chatbot\\snapshots\\b8e0ab4b7ef7c686a23654e24ffd6ebe36bb59da\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing VisionEncoderDecoderModel.\n",
      "\n",
      "All the weights of VisionEncoderDecoderModel were initialized from the model checkpoint at daekeun-ml/ko-trocr-base-nsmc-news-chatbot.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use VisionEncoderDecoderModel for predictions without further training.\n",
      "loading file vocab.txt from cache at C:\\Users\\Jo/.cache\\huggingface\\hub\\models--daekeun-ml--ko-trocr-base-nsmc-news-chatbot\\snapshots\\b8e0ab4b7ef7c686a23654e24ffd6ebe36bb59da\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\Jo/.cache\\huggingface\\hub\\models--daekeun-ml--ko-trocr-base-nsmc-news-chatbot\\snapshots\\b8e0ab4b7ef7c686a23654e24ffd6ebe36bb59da\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\Jo/.cache\\huggingface\\hub\\models--daekeun-ml--ko-trocr-base-nsmc-news-chatbot\\snapshots\\b8e0ab4b7ef7c686a23654e24ffd6ebe36bb59da\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Jo/.cache\\huggingface\\hub\\models--daekeun-ml--ko-trocr-base-nsmc-news-chatbot\\snapshots\\b8e0ab4b7ef7c686a23654e24ffd6ebe36bb59da\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "if FULL_TRAINING:\n",
    "    vision_hf_model = 'facebook/deit-base-distilled-patch16-384'\n",
    "    nlp_hf_model = \"klue/roberta-base\"\n",
    "    \n",
    "    # Reference: https://github.com/huggingface/transformers/issues/15823\n",
    "    # initialize the encoder from a pretrained ViT and the decoder from a pretrained BERT model. \n",
    "    # Note that the cross-attention layers will be randomly initialized, and need to be fine-tuned on a downstream dataset\n",
    "    model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(vision_hf_model, nlp_hf_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(nlp_hf_model)\n",
    "else:\n",
    "    trocr_model = 'daekeun-ml/ko-trocr-base-nsmc-news-chatbot'\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(trocr_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(trocr_model)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93156ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.1)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53dbdcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, df, processor, tokenizer, max_target_length=32):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(file_name).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text      \n",
    "        labels = self.tokenizer(text, padding=\"max_length\", \n",
    "                                stride=32,\n",
    "                                truncation=True,\n",
    "                                max_length=self.max_target_length).input_ids\n",
    "        \n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40ed10b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at C:\\Users\\Jo/.cache\\huggingface\\hub\\models--microsoft--trocr-base-handwritten\\snapshots\\69659a277424eb381574e4952f3b3fa3440a419b\\preprocessor_config.json\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "{param_name} should be a dictionary on of the following set of keys: {VALID_SIZE_DICT_KEYS}, got {size}. Converted to {size_dict}.\n",
      "Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 384,\n",
      "    \"width\": 384\n",
      "  }\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\Jo/.cache\\huggingface\\hub\\models--microsoft--trocr-base-handwritten\\snapshots\\69659a277424eb381574e4952f3b3fa3440a419b\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\Jo/.cache\\huggingface\\hub\\models--microsoft--trocr-base-handwritten\\snapshots\\69659a277424eb381574e4952f3b3fa3440a419b\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\Jo/.cache\\huggingface\\hub\\models--microsoft--trocr-base-handwritten\\snapshots\\69659a277424eb381574e4952f3b3fa3440a419b\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Jo/.cache\\huggingface\\hub\\models--microsoft--trocr-base-handwritten\\snapshots\\69659a277424eb381574e4952f3b3fa3440a419b\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 900\n",
      "Number of validation examples: 100\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "max_length = 64\n",
    "\n",
    "train_dataset = OCRDataset(\n",
    "    #dataset_dir=dataset_dir,\n",
    "    df=train_df,\n",
    "    tokenizer=tokenizer,\n",
    "    processor=processor,\n",
    "    max_target_length=max_length\n",
    ")\n",
    "eval_dataset = OCRDataset(\n",
    "    #dataset_dir=dataset_dir,\n",
    "    df=test_df,\n",
    "    tokenizer=tokenizer,\n",
    "    processor=processor,\n",
    "    max_target_length=max_length\n",
    ")\n",
    "\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbe21b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([3, 384, 384])\n",
      "labels torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=52x64 at 0x2B30BCFD520>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_idx = np.random.randint(len(train_df))\n",
    "encoding = train_dataset[img_idx]\n",
    "for k,v in encoding.items():\n",
    "    print(k, v.shape)\n",
    "    \n",
    "image = Image.open(train_df['file_name'][img_idx]).convert(\"RGB\")\n",
    "image    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e306a668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "애\n"
     ]
    }
   ],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = tokenizer.pad_token_id\n",
    "label_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7012636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.max_length = max_length\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c995d7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=4e-5,\n",
    "    output_dir=\"./\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=5000,\n",
    "    eval_steps=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2323d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "wer_metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a714255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer, \"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8a628cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1b156119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 164 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92b15b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jo\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 900\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 57\n",
      "  Number of trainable parameters = 225701120\n",
      "Trainer is attempting to log a value of \"{'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': True, 'cross_attention_hidden_size': None, 'add_cross_attention': True, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['RobertaForMaskedLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': 'BertTokenizer', 'prefix': None, 'bos_token_id': 0, 'pad_token_id': 1, 'eos_token_id': 2, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '', 'transformers_version': '4.25.1', 'gradient_checkpointing': False, 'vocab_size': 32000, 'hidden_size': 768, 'num_hidden_layers': 12, 'num_attention_heads': 12, 'hidden_act': 'gelu', 'intermediate_size': 3072, 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 514, 'type_vocab_size': 1, 'initializer_range': 0.02, 'layer_norm_eps': 1e-05, 'position_embedding_type': 'absolute', 'use_cache': True, 'classifier_dropout': None, 'model_type': 'roberta'}\" for key \"decoder\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute. You can use `MLFLOW_FLATTEN_PARAMS` environment variable to flatten the parameters and avoid this message.\n",
      "Trainer is attempting to log a value of \"{'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['DeiTForImageClassificationWithTeacher'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '', 'transformers_version': '4.25.1', 'hidden_size': 768, 'num_hidden_layers': 12, 'num_attention_heads': 12, 'intermediate_size': 3072, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.0, 'attention_probs_dropout_prob': 0.0, 'initializer_range': 0.02, 'layer_norm_eps': 1e-12, 'image_size': 384, 'patch_size': 16, 'num_channels': 3, 'qkv_bias': True, 'encoder_stride': 16, 'model_type': 'deit'}\" for key \"encoder\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute. You can use `MLFLOW_FLATTEN_PARAMS` environment variable to flatten the parameters and avoid this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 1:20:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbd4c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FULL_TRAINING:\n",
    "    steps = []\n",
    "    losses = []\n",
    "    for obj in trainer.state.log_history:\n",
    "        steps.append(obj['step'])\n",
    "        losses.append(obj['loss'])\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    f = plt.figure(figsize=(12,6))\n",
    "    plt.plot(steps, losses)\n",
    "    plt.xlabel('step')\n",
    "    plt.ylabel('training loss')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c24485bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./model\n",
      "Configuration saved in ./model\\config.json\n",
      "Model weights saved in ./model\\pytorch_model.bin\n",
      "tokenizer config file saved in ./model\\tokenizer_config.json\n",
      "Special tokens file saved in ./model\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(output_dir=\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8c64775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/7 00:55 < 04:35, 0.02 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28048/2611102455.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0meval_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer_seq2seq.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gen_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     def predict(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2810\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2811\u001b[1;33m         output = eval_loop(\n\u001b[0m\u001b[0;32m   2812\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2813\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Evaluation\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m             \u001b[1;31m# Prediction step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2989\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2990\u001b[0m             \u001b[0minputs_decode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minclude_inputs_for_metrics\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer_seq2seq.py\u001b[0m in \u001b[0;36mprediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_smoother\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_smoother\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You have to specify pixel_values\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    582\u001b[0m                 \u001b[0mpixel_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deit\\modeling_deit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool_masked_pos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool_masked_pos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    527\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deit\\modeling_deit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    369\u001b[0m                 )\n\u001b[0;32m    370\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m                 \u001b[0mlayer_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deit\\modeling_deit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;31m# second residual connection is done here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\deit\\modeling_deit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    eval_result = trainer.evaluate(eval_dataset, max_length=64)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a21ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62c9216f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=66x64 at 0x2B310002610>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "img_paths = glob.glob('../test/*.png')\n",
    "img_idx = np.random.randint(len(img_paths))\n",
    "image = Image.open(img_paths[img_idx])\n",
    "#img_idx = np.random.randint(len(eval_dataset))\n",
    "#image = Image.open(eval_dataset.dataset_dir + train_df['file_name'][img_idx])\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e7c479ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74121"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a5d7324a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jo\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 64 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[   0,    0, 1506, 2728,    2]]), '인것')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "pixel_values = (processor(image, return_tensors=\"pt\").pixel_values)\n",
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] \n",
    "generated_ids, generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8919cb",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d138a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████▍                                                | 6698/20000 [4:40:28<9:07:11,  2.47s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "preds = []\n",
    "\n",
    "for path in tqdm(img_paths[:20000]):\n",
    "    image = Image.open(path)\n",
    "    pixel_values = (processor(image, return_tensors=\"pt\").pixel_values)\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] \n",
    "    preds.append(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90706586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>가</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id label\n",
       "0  TEST_00000     가\n",
       "1  TEST_00001     가\n",
       "2  TEST_00002     가\n",
       "3  TEST_00003     가\n",
       "4  TEST_00004     가"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(\"../sample_submission.csv\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7882785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['닭닭',\n",
       " '삶상',\n",
       " '받다',\n",
       " '바꾸다',\n",
       " '실',\n",
       " '배타다',\n",
       " '인식하다',\n",
       " '센터',\n",
       " '쇼핑몰',\n",
       " '광주',\n",
       " '그대',\n",
       " '무청',\n",
       " '도구',\n",
       " '승',\n",
       " '괴로워하다',\n",
       " '카드',\n",
       " '답답하다',\n",
       " '감사하다',\n",
       " '달자',\n",
       " '가스하다']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d3cd6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_65647</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_65647.png</td>\n",
       "      <td>던</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_22304</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_22304.png</td>\n",
       "      <td>갚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_28921</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_28921.png</td>\n",
       "      <td>지적</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_21995</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_21995.png</td>\n",
       "      <td>맘대로</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_61489</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_61489.png</td>\n",
       "      <td>면</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TRAIN_29108</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_29108.png</td>\n",
       "      <td>좀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TRAIN_48261</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_48261.png</td>\n",
       "      <td>아버지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TRAIN_62937</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_62937.png</td>\n",
       "      <td>음성</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TRAIN_08504</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_08504.png</td>\n",
       "      <td>향기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TRAIN_22583</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_22583.png</td>\n",
       "      <td>만</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TRAIN_73244</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_73244.png</td>\n",
       "      <td>쏜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TRAIN_04413</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_04413.png</td>\n",
       "      <td>희망하다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TRAIN_70384</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_70384.png</td>\n",
       "      <td>각</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TRAIN_37475</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_37475.png</td>\n",
       "      <td>당연하다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TRAIN_37813</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_37813.png</td>\n",
       "      <td>꾸준하다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TRAIN_71271</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_71271.png</td>\n",
       "      <td>테스트</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TRAIN_20312</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_20312.png</td>\n",
       "      <td>기적</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TRAIN_37400</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_37400.png</td>\n",
       "      <td>게</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TRAIN_01520</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_01520.png</td>\n",
       "      <td>짯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TRAIN_07978</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_07978.png</td>\n",
       "      <td>밀접하다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TRAIN_68855</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_68855.png</td>\n",
       "      <td>바치다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TRAIN_11045</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_11045.png</td>\n",
       "      <td>민</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TRAIN_67818</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_67818.png</td>\n",
       "      <td>울음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TRAIN_65634</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_65634.png</td>\n",
       "      <td>그려지다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TRAIN_01583</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_01583.png</td>\n",
       "      <td>강도</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TRAIN_04663</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_04663.png</td>\n",
       "      <td>한편</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TRAIN_06211</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_06211.png</td>\n",
       "      <td>역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TRAIN_14536</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_14536.png</td>\n",
       "      <td>출석하다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>TRAIN_57907</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_57907.png</td>\n",
       "      <td>생활비</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>TRAIN_26164</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_26164.png</td>\n",
       "      <td>꺾다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>TRAIN_68223</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_68223.png</td>\n",
       "      <td>쥔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>TRAIN_18463</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_18463.png</td>\n",
       "      <td>공중</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>TRAIN_16671</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_16671.png</td>\n",
       "      <td>광</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>TRAIN_34226</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_34226.png</td>\n",
       "      <td>컁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>TRAIN_12691</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_12691.png</td>\n",
       "      <td>줄</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>TRAIN_16117</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_16117.png</td>\n",
       "      <td>동서남북</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>TRAIN_70569</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_70569.png</td>\n",
       "      <td>지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>TRAIN_65173</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_65173.png</td>\n",
       "      <td>삣</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>TRAIN_17168</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_17168.png</td>\n",
       "      <td>킬로미터</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>TRAIN_20050</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_20050.png</td>\n",
       "      <td>자율</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>TRAIN_71722</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_71722.png</td>\n",
       "      <td>재밌다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>TRAIN_34059</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_34059.png</td>\n",
       "      <td>스물</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>TRAIN_73791</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_73791.png</td>\n",
       "      <td>넌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>TRAIN_57507</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_57507.png</td>\n",
       "      <td>파출소</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>TRAIN_64053</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_64053.png</td>\n",
       "      <td>부분</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>TRAIN_41793</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_41793.png</td>\n",
       "      <td>쳐다보다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>TRAIN_20961</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_20961.png</td>\n",
       "      <td>터미널</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>TRAIN_68543</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_68543.png</td>\n",
       "      <td>특수성</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>TRAIN_13005</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_13005.png</td>\n",
       "      <td>지나다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>TRAIN_08018</td>\n",
       "      <td>C:/Users/Jo/PYDATAexam/train/TRAIN_08018.png</td>\n",
       "      <td>흔들다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                     file_name  text\n",
       "0   TRAIN_65647  C:/Users/Jo/PYDATAexam/train/TRAIN_65647.png     던\n",
       "1   TRAIN_22304  C:/Users/Jo/PYDATAexam/train/TRAIN_22304.png     갚\n",
       "2   TRAIN_28921  C:/Users/Jo/PYDATAexam/train/TRAIN_28921.png    지적\n",
       "3   TRAIN_21995  C:/Users/Jo/PYDATAexam/train/TRAIN_21995.png   맘대로\n",
       "4   TRAIN_61489  C:/Users/Jo/PYDATAexam/train/TRAIN_61489.png     면\n",
       "5   TRAIN_29108  C:/Users/Jo/PYDATAexam/train/TRAIN_29108.png     좀\n",
       "6   TRAIN_48261  C:/Users/Jo/PYDATAexam/train/TRAIN_48261.png   아버지\n",
       "7   TRAIN_62937  C:/Users/Jo/PYDATAexam/train/TRAIN_62937.png    음성\n",
       "8   TRAIN_08504  C:/Users/Jo/PYDATAexam/train/TRAIN_08504.png    향기\n",
       "9   TRAIN_22583  C:/Users/Jo/PYDATAexam/train/TRAIN_22583.png     만\n",
       "10  TRAIN_73244  C:/Users/Jo/PYDATAexam/train/TRAIN_73244.png     쏜\n",
       "11  TRAIN_04413  C:/Users/Jo/PYDATAexam/train/TRAIN_04413.png  희망하다\n",
       "12  TRAIN_70384  C:/Users/Jo/PYDATAexam/train/TRAIN_70384.png     각\n",
       "13  TRAIN_37475  C:/Users/Jo/PYDATAexam/train/TRAIN_37475.png  당연하다\n",
       "14  TRAIN_37813  C:/Users/Jo/PYDATAexam/train/TRAIN_37813.png  꾸준하다\n",
       "15  TRAIN_71271  C:/Users/Jo/PYDATAexam/train/TRAIN_71271.png   테스트\n",
       "16  TRAIN_20312  C:/Users/Jo/PYDATAexam/train/TRAIN_20312.png    기적\n",
       "17  TRAIN_37400  C:/Users/Jo/PYDATAexam/train/TRAIN_37400.png     게\n",
       "18  TRAIN_01520  C:/Users/Jo/PYDATAexam/train/TRAIN_01520.png     짯\n",
       "19  TRAIN_07978  C:/Users/Jo/PYDATAexam/train/TRAIN_07978.png  밀접하다\n",
       "20  TRAIN_68855  C:/Users/Jo/PYDATAexam/train/TRAIN_68855.png   바치다\n",
       "21  TRAIN_11045  C:/Users/Jo/PYDATAexam/train/TRAIN_11045.png     민\n",
       "22  TRAIN_67818  C:/Users/Jo/PYDATAexam/train/TRAIN_67818.png    울음\n",
       "23  TRAIN_65634  C:/Users/Jo/PYDATAexam/train/TRAIN_65634.png  그려지다\n",
       "24  TRAIN_01583  C:/Users/Jo/PYDATAexam/train/TRAIN_01583.png    강도\n",
       "25  TRAIN_04663  C:/Users/Jo/PYDATAexam/train/TRAIN_04663.png    한편\n",
       "26  TRAIN_06211  C:/Users/Jo/PYDATAexam/train/TRAIN_06211.png     역\n",
       "27  TRAIN_14536  C:/Users/Jo/PYDATAexam/train/TRAIN_14536.png  출석하다\n",
       "28  TRAIN_57907  C:/Users/Jo/PYDATAexam/train/TRAIN_57907.png   생활비\n",
       "29  TRAIN_26164  C:/Users/Jo/PYDATAexam/train/TRAIN_26164.png    꺾다\n",
       "30  TRAIN_68223  C:/Users/Jo/PYDATAexam/train/TRAIN_68223.png     쥔\n",
       "31  TRAIN_18463  C:/Users/Jo/PYDATAexam/train/TRAIN_18463.png    공중\n",
       "32  TRAIN_16671  C:/Users/Jo/PYDATAexam/train/TRAIN_16671.png     광\n",
       "33  TRAIN_34226  C:/Users/Jo/PYDATAexam/train/TRAIN_34226.png     컁\n",
       "34  TRAIN_12691  C:/Users/Jo/PYDATAexam/train/TRAIN_12691.png     줄\n",
       "35  TRAIN_16117  C:/Users/Jo/PYDATAexam/train/TRAIN_16117.png  동서남북\n",
       "36  TRAIN_70569  C:/Users/Jo/PYDATAexam/train/TRAIN_70569.png     지\n",
       "37  TRAIN_65173  C:/Users/Jo/PYDATAexam/train/TRAIN_65173.png     삣\n",
       "38  TRAIN_17168  C:/Users/Jo/PYDATAexam/train/TRAIN_17168.png  킬로미터\n",
       "39  TRAIN_20050  C:/Users/Jo/PYDATAexam/train/TRAIN_20050.png    자율\n",
       "40  TRAIN_71722  C:/Users/Jo/PYDATAexam/train/TRAIN_71722.png   재밌다\n",
       "41  TRAIN_34059  C:/Users/Jo/PYDATAexam/train/TRAIN_34059.png    스물\n",
       "42  TRAIN_73791  C:/Users/Jo/PYDATAexam/train/TRAIN_73791.png     넌\n",
       "43  TRAIN_57507  C:/Users/Jo/PYDATAexam/train/TRAIN_57507.png   파출소\n",
       "44  TRAIN_64053  C:/Users/Jo/PYDATAexam/train/TRAIN_64053.png    부분\n",
       "45  TRAIN_41793  C:/Users/Jo/PYDATAexam/train/TRAIN_41793.png  쳐다보다\n",
       "46  TRAIN_20961  C:/Users/Jo/PYDATAexam/train/TRAIN_20961.png   터미널\n",
       "47  TRAIN_68543  C:/Users/Jo/PYDATAexam/train/TRAIN_68543.png   특수성\n",
       "48  TRAIN_13005  C:/Users/Jo/PYDATAexam/train/TRAIN_13005.png   지나다\n",
       "49  TRAIN_08018  C:/Users/Jo/PYDATAexam/train/TRAIN_08018.png   흔들다"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a3401b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
